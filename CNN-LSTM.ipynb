{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fd8437-727e-4f1b-912a-910479e3ae84",
   "metadata": {},
   "source": [
    "This section imports all necessary libraries and loads your data. It also creates the two different target variables:\n",
    "\n",
    "y_stage1: Binary (0 for 'Normal', 1 for 'Fault')\n",
    "\n",
    "y_stage2: Multiclass (0 for 'Normal', 1-20 for each fault type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683a122d-7471-4564-aed6-77cb61635242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data prepared with 37 common features.\n",
      "Scaling data...\n",
      "\n",
      "--- Preparing CNN Data ---\n",
      "Original features: 37. Padding to 49 for a (7, 7) image.\n",
      "\n",
      "--- Training CNN Stage 1 (Detection) ---\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 5, 5, 32)          320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 2, 2, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,961\n",
      "Trainable params: 16,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 2s 6ms/step - loss: 0.2097 - accuracy: 0.9371 - val_loss: 0.0729 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.1954 - accuracy: 0.9383 - val_loss: 0.0955 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1903 - accuracy: 0.9383 - val_loss: 0.0944 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.1870 - accuracy: 0.9383 - val_loss: 0.1069 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1835 - accuracy: 0.9383 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.1827 - accuracy: 0.9383 - val_loss: 0.0859 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.1824 - accuracy: 0.9383 - val_loss: 0.1004 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.1798 - accuracy: 0.9383 - val_loss: 0.1090 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.1790 - accuracy: 0.9383 - val_loss: 0.1141 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.1794 - accuracy: 0.9383 - val_loss: 0.1407 - val_accuracy: 1.0000\n",
      "\n",
      "--- CNN Stage 1 Evaluation ---\n",
      "1587/1587 [==============================] - 3s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADITI\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ADITI\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ADITI\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2820\n",
      "           1       0.94      1.00      0.97     47940\n",
      "\n",
      "    accuracy                           0.94     50760\n",
      "   macro avg       0.47      0.50      0.49     50760\n",
      "weighted avg       0.89      0.94      0.92     50760\n",
      "\n",
      "\n",
      "--- Preparing LSTM Data ---\n",
      "\n",
      "--- Training LSTM Stage 1 (Detection) ---\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 50)            17600     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,401\n",
      "Trainable params: 40,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 5s 18ms/step - loss: 0.1988 - accuracy: 0.9343 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1407 - accuracy: 0.9387 - val_loss: 0.1028 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.1184 - accuracy: 0.9387 - val_loss: 0.1218 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.1075 - accuracy: 0.9388 - val_loss: 0.0983 - val_accuracy: 0.9981\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1007 - accuracy: 0.9396 - val_loss: 0.1285 - val_accuracy: 0.9749\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0946 - accuracy: 0.9431 - val_loss: 0.1163 - val_accuracy: 0.9398\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0901 - accuracy: 0.9488 - val_loss: 0.1305 - val_accuracy: 0.9197\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0862 - accuracy: 0.9524 - val_loss: 0.1333 - val_accuracy: 0.8904\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.0836 - accuracy: 0.9560 - val_loss: 0.1376 - val_accuracy: 0.8869\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0795 - accuracy: 0.9583 - val_loss: 0.1746 - val_accuracy: 0.8537\n",
      "\n",
      "--- LSTM Stage 1 Evaluation ---\n",
      "1586/1586 [==============================] - 6s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.64      0.29      2810\n",
      "           1       0.98      0.84      0.90     47940\n",
      "\n",
      "    accuracy                           0.83     50750\n",
      "   macro avg       0.58      0.74      0.59     50750\n",
      "weighted avg       0.93      0.83      0.87     50750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_factors(n):\n",
    "    \"\"\"Calculates two factors for reshaping (e.g., 51 -> (17, 3))\"\"\"\n",
    "    for i in range(int(np.sqrt(n)), 0, -1):\n",
    "        if n % i == 0:\n",
    "            return (n // i, i)\n",
    "    return (n, 1) # Fallback for prime numbers\n",
    "\n",
    "def create_cnn_dataset(X, img_rows, img_cols):\n",
    "    \"\"\"Reshapes 1D feature vector into 2D image\"\"\"\n",
    "    if X.shape[1] != img_rows * img_cols:\n",
    "        raise ValueError(f\"Cannot reshape {X.shape[1]} features into ({img_rows}, {img_cols})\")\n",
    "    return X.reshape((X.shape[0], img_rows, img_cols, 1))\n",
    "\n",
    "def create_lstm_dataset(X, y, time_steps=10):\n",
    "    \"\"\"Converts data into sequences for LSTM\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "    \n",
    "def pad_features(X, target_features):\n",
    "    \"\"\"Pads feature vector X to have target_features columns\"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    pad_width = target_features - num_features\n",
    "    \n",
    "    if pad_width < 0:\n",
    "        # This shouldn't happen, but good to check\n",
    "        return X[:, :target_features]\n",
    "        \n",
    "    # Create a padding array of zeros\n",
    "    padding = np.zeros((num_samples, pad_width))\n",
    "    # Concatenate original features with padding\n",
    "    return np.concatenate([X, padding], axis=1)\n",
    "\n",
    "# --- 1. DATA PREPARATION (Using your code) ---\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('modified_data_train.csv')\n",
    "test_data  = pd.read_csv('modified_data_test.csv')\n",
    "\n",
    "train_data['binary_fault'] = train_data['faultNumber'].apply(lambda x: 0 if x == 0 else 1)\n",
    "test_data['binary_fault'] = test_data['faultNumber'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "drop_cols = ['faultNumber', 'simulationRun', 'sample','binary_fault']\n",
    "\n",
    "X_train = train_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_train = train_data['binary_fault']\n",
    "\n",
    "X_test = test_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_test = test_data['binary_fault']\n",
    "\n",
    "# Find common columns to fix potential mismatch\n",
    "common_cols = sorted(list(set(X_train.columns).intersection(set(X_test.columns))))\n",
    "\n",
    "X_train = X_train[common_cols]\n",
    "X_test  = X_test[common_cols]\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(f\"Data prepared with {len(common_cols)} common features.\")\n",
    "\n",
    "# --- 2. SCALING ---\n",
    "print(\"Scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "N_FEATURES = X_train_scaled.shape[1] # This will be 37\n",
    "\n",
    "# --- 3. MODEL 1: CNN FOR STAGE 1 (DETECTION) --- [CORRECTED SECTION]\n",
    "\n",
    "print(\"\\n--- Preparing CNN Data ---\")\n",
    "\n",
    "# Find next perfect square to pad to\n",
    "TARGET_DIM = int(np.ceil(np.sqrt(N_FEATURES))) # ceil(sqrt(37)) = 7\n",
    "IMG_ROWS, IMG_COLS = TARGET_DIM, TARGET_DIM     # 7, 7\n",
    "TARGET_FEATURES = TARGET_DIM * TARGET_DIM      # 49\n",
    "\n",
    "print(f\"Original features: {N_FEATURES}. Padding to {TARGET_FEATURES} for a ({IMG_ROWS}, {IMG_COLS}) image.\")\n",
    "\n",
    "# Pad the scaled data\n",
    "X_train_padded = pad_features(X_train_scaled, TARGET_FEATURES)\n",
    "X_test_padded = pad_features(X_test_scaled, TARGET_FEATURES)\n",
    "\n",
    "CNN_INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1) # (7, 7, 1)\n",
    "\n",
    "# Use the *padded* data to create the CNN dataset\n",
    "X_train_cnn = create_cnn_dataset(X_train_padded, IMG_ROWS, IMG_COLS)\n",
    "X_test_cnn = create_cnn_dataset(X_test_padded, IMG_ROWS, IMG_COLS)\n",
    "\n",
    "\n",
    "def build_cnn_stage1(input_shape):\n",
    "    model = Sequential()\n",
    "    # This layer now works with input_shape=(7, 7, 1)\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training CNN Stage 1 (Detection) ---\")\n",
    "cnn_model_s1 = build_cnn_stage1(CNN_INPUT_SHAPE)\n",
    "cnn_model_s1.summary()\n",
    "\n",
    "# Fit on the padded CNN data\n",
    "cnn_model_s1.fit(X_train_cnn, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=10,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "print(\"\\n--- CNN Stage 1 Evaluation ---\")\n",
    "# Predict on the padded test data\n",
    "y_pred_cnn_s1 = (cnn_model_s1.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred_cnn_s1))\n",
    "\n",
    "# --- 4. MODEL 2: LSTM FOR STAGE 1 (DETECTION) --- [UNCHANGED]\n",
    "\n",
    "print(\"\\n--- Preparing LSTM Data ---\")\n",
    "TIME_STEPS = 10\n",
    "# Use the *original* N_FEATURES (37) for the LSTM\n",
    "LSTM_INPUT_SHAPE = (TIME_STEPS, N_FEATURES) \n",
    "\n",
    "# Create sequences from the *original, unpadded* scaled data\n",
    "X_train_lstm, y_train_lstm = create_lstm_dataset(X_train_scaled, y_train.values, TIME_STEPS)\n",
    "X_test_lstm, y_test_lstm = create_lstm_dataset(X_test_scaled, y_test.values, TIME_STEPS)\n",
    "\n",
    "def build_lstm_stage1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training LSTM Stage 1 (Detection) ---\")\n",
    "lstm_model_s1 = build_lstm_stage1(LSTM_INPUT_SHAPE)\n",
    "lstm_model_s1.summary()\n",
    "\n",
    "lstm_model_s1.fit(X_train_lstm, y_train_lstm,\n",
    "                  batch_size=128,\n",
    "                  epochs=10,\n",
    "                  validation_split=0.1)\n",
    "                  \n",
    "print(\"\\n--- LSTM Stage 1 Evaluation ---\")\n",
    "y_pred_lstm_s1 = (lstm_model_s1.predict(X_test_lstm) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test_lstm, y_pred_lstm_s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7839df-3335-4b51-bf9f-9ad0c7b4373e",
   "metadata": {},
   "source": [
    "We can see that due to the imbalance present in the data which results in low precision and f1-score too. Now, to tackle this we could use oversampling, undersampling or SMOTE technique too but as the architecture of the model is complex thus using these techniques may result in slow model performance. Thus we are using class weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19ba5fc-0c1b-481b-ae9e-29731ea7a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data prepared with 37 common features.\n",
      "Scaling data...\n",
      "Calculating class weights...\n",
      "Class weights calculated: {0: 9.0, 1: 0.5294117647058824}\n",
      "\n",
      "--- Preparing CNN Data ---\n",
      "Original features: 37. Padding to 49 for a (7, 7) image.\n",
      "\n",
      "--- Training CNN Stage 1 (Detection) ---\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 5, 5, 32)          320       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 2, 2, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,961\n",
      "Trainable params: 16,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 2s 6ms/step - loss: 0.6095 - accuracy: 0.6593 - auc: 0.7414 - val_loss: 0.7878 - val_accuracy: 0.3715 - val_auc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.5090 - accuracy: 0.6333 - auc: 0.8057 - val_loss: 0.7958 - val_accuracy: 0.4028 - val_auc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4796 - accuracy: 0.6500 - auc: 0.8224 - val_loss: 0.8543 - val_accuracy: 0.4001 - val_auc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4648 - accuracy: 0.6486 - auc: 0.8288 - val_loss: 0.7672 - val_accuracy: 0.4333 - val_auc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4537 - accuracy: 0.6648 - auc: 0.8370 - val_loss: 0.9140 - val_accuracy: 0.4062 - val_auc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.4437 - accuracy: 0.6721 - auc: 0.8432 - val_loss: 0.9315 - val_accuracy: 0.4035 - val_auc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.4367 - accuracy: 0.6782 - auc: 0.8476 - val_loss: 0.9230 - val_accuracy: 0.4182 - val_auc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.4358 - accuracy: 0.6758 - auc: 0.8481 - val_loss: 0.8374 - val_accuracy: 0.4302 - val_auc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.4347 - accuracy: 0.6765 - auc: 0.8476 - val_loss: 0.9240 - val_accuracy: 0.4101 - val_auc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.4264 - accuracy: 0.6843 - auc: 0.8527 - val_loss: 0.8646 - val_accuracy: 0.4275 - val_auc: 0.0000e+00\n",
      "\n",
      "--- CNN Stage 1 Evaluation ---\n",
      "1587/1587 [==============================] - 3s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.98      0.20      2820\n",
      "           1       1.00      0.54      0.70     47940\n",
      "\n",
      "    accuracy                           0.57     50760\n",
      "   macro avg       0.55      0.76      0.45     50760\n",
      "weighted avg       0.95      0.57      0.67     50760\n",
      "\n",
      "\n",
      "--- Preparing LSTM Data ---\n",
      "\n",
      "--- Training LSTM Stage 1 (Detection) ---\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 10, 50)            17600     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,401\n",
      "Trainable params: 40,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 6s 21ms/step - loss: 0.4450 - accuracy: 0.6993 - auc: 0.8497 - val_loss: 0.5919 - val_accuracy: 0.6434 - val_auc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.3171 - accuracy: 0.7856 - auc: 0.9137 - val_loss: 0.6088 - val_accuracy: 0.6550 - val_auc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.2512 - accuracy: 0.8439 - auc: 0.9436 - val_loss: 0.8223 - val_accuracy: 0.6592 - val_auc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.2254 - accuracy: 0.8675 - auc: 0.9476 - val_loss: 0.5350 - val_accuracy: 0.7538 - val_auc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.2050 - accuracy: 0.8820 - auc: 0.9544 - val_loss: 0.6710 - val_accuracy: 0.7036 - val_auc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.1894 - accuracy: 0.8922 - auc: 0.9583 - val_loss: 0.4409 - val_accuracy: 0.7843 - val_auc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.1794 - accuracy: 0.8990 - auc: 0.9625 - val_loss: 0.6088 - val_accuracy: 0.7569 - val_auc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1844 - accuracy: 0.8966 - auc: 0.9627 - val_loss: 0.6615 - val_accuracy: 0.7364 - val_auc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1607 - accuracy: 0.9104 - auc: 0.9690 - val_loss: 0.5682 - val_accuracy: 0.7673 - val_auc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.1524 - accuracy: 0.9166 - auc: 0.9712 - val_loss: 0.6788 - val_accuracy: 0.7596 - val_auc: 0.0000e+00\n",
      "\n",
      "--- LSTM Stage 1 Evaluation ---\n",
      "1586/1586 [==============================] - 8s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.88      0.30      2810\n",
      "           1       0.99      0.76      0.86     47940\n",
      "\n",
      "    accuracy                           0.77     50750\n",
      "   macro avg       0.58      0.82      0.58     50750\n",
      "weighted avg       0.95      0.77      0.83     50750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_factors(n):\n",
    "    for i in range(int(np.sqrt(n)), 0, -1):\n",
    "        if n % i == 0:\n",
    "            return (n // i, i)\n",
    "    return (n, 1)\n",
    "\n",
    "def create_cnn_dataset(X, img_rows, img_cols):\n",
    "    if X.shape[1] != img_rows * img_cols:\n",
    "        raise ValueError(f\"Cannot reshape {X.shape[1]} features into ({img_rows}, {img_cols})\")\n",
    "    return X.reshape((X.shape[0], img_rows, img_cols, 1))\n",
    "\n",
    "def create_lstm_dataset(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "    \n",
    "def pad_features(X, target_features):\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    pad_width = target_features - num_features\n",
    "    if pad_width < 0:\n",
    "        return X[:, :target_features]\n",
    "    padding = np.zeros((num_samples, pad_width))\n",
    "    return np.concatenate([X, padding], axis=1)\n",
    "\n",
    "# --- 1. DATA PREPARATION ---\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('modified_data_train.csv')\n",
    "test_data  = pd.read_csv('modified_data_test.csv')\n",
    "\n",
    "train_data['binary_fault'] = train_data['faultNumber'].apply(lambda x: 0 if x == 0 else 1)\n",
    "test_data['binary_fault'] = test_data['faultNumber'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "drop_cols = ['faultNumber', 'simulationRun', 'sample','binary_fault']\n",
    "\n",
    "X_train = train_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_train = train_data['binary_fault']\n",
    "\n",
    "X_test = test_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_test = test_data['binary_fault']\n",
    "\n",
    "common_cols = sorted(list(set(X_train.columns).intersection(set(X_test.columns))))\n",
    "X_train = X_train[common_cols]\n",
    "X_test  = X_test[common_cols]\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(f\"Data prepared with {len(common_cols)} common features.\")\n",
    "\n",
    "# --- 2. SCALING ---\n",
    "print(\"Scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "N_FEATURES = X_train_scaled.shape[1]\n",
    "\n",
    "# --- [NEW] CALCULATE CLASS WEIGHTS ---\n",
    "print(\"Calculating class weights...\")\n",
    "weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(zip(np.unique(y_train), weights))\n",
    "print(f\"Class weights calculated: {class_weights_dict}\")\n",
    "\n",
    "# --- 3. MODEL 1: CNN FOR STAGE 1 (DETECTION) ---\n",
    "\n",
    "print(\"\\n--- Preparing CNN Data ---\")\n",
    "TARGET_DIM = int(np.ceil(np.sqrt(N_FEATURES)))\n",
    "IMG_ROWS, IMG_COLS = TARGET_DIM, TARGET_DIM\n",
    "TARGET_FEATURES = TARGET_DIM * TARGET_DIM\n",
    "\n",
    "print(f\"Original features: {N_FEATURES}. Padding to {TARGET_FEATURES} for a ({IMG_ROWS}, {IMG_COLS}) image.\")\n",
    "\n",
    "X_train_padded = pad_features(X_train_scaled, TARGET_FEATURES)\n",
    "X_test_padded = pad_features(X_test_scaled, TARGET_FEATURES)\n",
    "\n",
    "CNN_INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "\n",
    "X_train_cnn = create_cnn_dataset(X_train_padded, IMG_ROWS, IMG_COLS)\n",
    "X_test_cnn = create_cnn_dataset(X_test_padded, IMG_ROWS, IMG_COLS)\n",
    "\n",
    "def build_cnn_stage1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', AUC(name='auc')]) # Added AUC\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training CNN Stage 1 (Detection) ---\")\n",
    "cnn_model_s1 = build_cnn_stage1(CNN_INPUT_SHAPE)\n",
    "cnn_model_s1.summary()\n",
    "\n",
    "cnn_model_s1.fit(X_train_cnn, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=10,\n",
    "                 validation_split=0.1,\n",
    "                 class_weight=class_weights_dict) # <-- ADDED CLASS WEIGHTS\n",
    "\n",
    "print(\"\\n--- CNN Stage 1 Evaluation ---\")\n",
    "y_pred_cnn_s1 = (cnn_model_s1.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred_cnn_s1))\n",
    "\n",
    "# --- 4. MODEL 2: LSTM FOR STAGE 1 (DETECTION) ---\n",
    "\n",
    "print(\"\\n--- Preparing LSTM Data ---\")\n",
    "TIME_STEPS = 10\n",
    "LSTM_INPUT_SHAPE = (TIME_STEPS, N_FEATURES) \n",
    "\n",
    "X_train_lstm, y_train_lstm = create_lstm_dataset(X_train_scaled, y_train.values, TIME_STEPS)\n",
    "X_test_lstm, y_test_lstm = create_lstm_dataset(X_test_scaled, y_test.values, TIME_STEPS)\n",
    "\n",
    "# Calculate weights for the time-stepped data\n",
    "weights_lstm = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_lstm),\n",
    "    y=y_train_lstm\n",
    ")\n",
    "class_weights_dict_lstm = dict(zip(np.unique(y_train_lstm), weights_lstm))\n",
    "\n",
    "def build_lstm_stage1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', AUC(name='auc')]) # Added AUC\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training LSTM Stage 1 (Detection) ---\")\n",
    "lstm_model_s1 = build_lstm_stage1(LSTM_INPUT_SHAPE)\n",
    "lstm_model_s1.summary()\n",
    "\n",
    "lstm_model_s1.fit(X_train_lstm, y_train_lstm,\n",
    "                  batch_size=128,\n",
    "                  epochs=10,\n",
    "                  validation_split=0.1,\n",
    "                  class_weight=class_weights_dict_lstm) # <-- ADDED CLASS WEIGHTS\n",
    "                  \n",
    "print(\"\\n--- LSTM Stage 1 Evaluation ---\")\n",
    "y_pred_lstm_s1 = (lstm_model_s1.predict(X_test_lstm) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test_lstm, y_pred_lstm_s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e14c6-9575-4615-84a4-829dda6bb2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping # <--- IMPORT\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_factors(n):\n",
    "    for i in range(int(np.sqrt(n)), 0, -1):\n",
    "        if n % i == 0:\n",
    "            return (n // i, i)\n",
    "    return (n, 1)\n",
    "\n",
    "def create_cnn_dataset(X, img_rows, img_cols):\n",
    "    if X.shape[1] != img_rows * img_cols:\n",
    "        raise ValueError(f\"Cannot reshape {X.shape[1]} features into ({img_rows}, {img_cols})\")\n",
    "    return X.reshape((X.shape[0], img_rows, img_cols, 1))\n",
    "\n",
    "def create_lstm_dataset(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "    \n",
    "def pad_features(X, target_features):\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    pad_width = target_features - num_features\n",
    "    if pad_width < 0:\n",
    "        return X[:, :target_features]\n",
    "    padding = np.zeros((num_samples, pad_width))\n",
    "    return np.concatenate([X, padding], axis=1)\n",
    "\n",
    "# --- 1. DATA PREPARATION ---\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('modified_data_train.csv')\n",
    "test_data  = pd.read_csv('modified_data_test.csv')\n",
    "\n",
    "train_data['binary_fault'] = train_data['faultNumber'].apply(lambda x: 0 if x == 0 else 1)\n",
    "test_data['binary_fault'] = test_data['faultNumber'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "drop_cols = ['faultNumber', 'simulationRun', 'sample','binary_fault']\n",
    "\n",
    "X_train = train_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_train = train_data['binary_fault']\n",
    "\n",
    "X_test = test_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_test = test_data['binary_fault']\n",
    "\n",
    "common_cols = sorted(list(set(X_train.columns).intersection(set(X_test.columns))))\n",
    "X_train = X_train[common_cols]\n",
    "X_test  = X_test[common_cols]\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(f\"Data prepared with {len(common_cols)} common features.\")\n",
    "\n",
    "# --- 2. SCALING ---\n",
    "print(\"Scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "N_FEATURES = X_train_scaled.shape[1]\n",
    "\n",
    "# --- CALCULATE CLASS WEIGHTS ---\n",
    "print(\"Calculating class weights...\")\n",
    "weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(zip(np.unique(y_train), weights))\n",
    "print(f\"Class weights calculated: {class_weights_dict}\")\n",
    "\n",
    "# --- [NEW] DEFINE EARLY STOPPING ---\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss', # Watch the validation loss\n",
    "    patience=3,         # Stop if it doesn't improve for 3 epochs\n",
    "    verbose=1,\n",
    "    restore_best_weights=True # Restore the best model weights\n",
    ")\n",
    "\n",
    "# --- 3. MODEL 1: CNN FOR STAGE 1 (DETECTION) ---\n",
    "\n",
    "print(\"\\n--- Preparing CNN Data ---\")\n",
    "TARGET_DIM = int(np.ceil(np.sqrt(N_FEATURES)))\n",
    "IMG_ROWS, IMG_COLS = TARGET_DIM, TARGET_DIM\n",
    "TARGET_FEATURES = TARGET_DIM * TARGET_DIM\n",
    "\n",
    "print(f\"Original features: {N_FEATURES}. Padding to {TARGET_FEATURES} for a ({IMG_ROWS}, {IMG_COLS}) image.\")\n",
    "\n",
    "X_train_padded = pad_features(X_train_scaled, TARGET_FEATURES)\n",
    "X_test_padded = pad_features(X_test_scaled, TARGET_FEATURES)\n",
    "\n",
    "CNN_INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "\n",
    "X_train_cnn = create_cnn_dataset(X_train_padded, IMG_ROWS, IMG_COLS)\n",
    "X_test_cnn = create_cnn_dataset(X_test_padded, IMG_ROWS, IMG_COLS)\n",
    "\n",
    "def build_cnn_stage1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training CNN Stage 1 (Detection) ---\")\n",
    "cnn_model_s1 = build_cnn_stage1(CNN_INPUT_SHAPE)\n",
    "cnn_model_s1.summary()\n",
    "\n",
    "cnn_model_s1.fit(X_train_cnn, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=20, # Increase epochs, EarlyStopping will handle it\n",
    "                 validation_split=0.1,\n",
    "                 class_weight=class_weights_dict,\n",
    "                 callbacks=[early_stopper]) # <-- ADDED CALLBACK\n",
    "\n",
    "print(\"\\n--- CNN Stage 1 Evaluation ---\")\n",
    "y_pred_cnn_s1 = (cnn_model_s1.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred_cnn_s1))\n",
    "\n",
    "# --- 4. MODEL 2: LSTM FOR STAGE 1 (DETECTION) ---\n",
    "\n",
    "print(\"\\n--- Preparing LSTM Data ---\")\n",
    "TIME_STEPS = 10\n",
    "LSTM_INPUT_SHAPE = (TIME_STEPS, N_FEATURES) \n",
    "\n",
    "X_train_lstm, y_train_lstm = create_lstm_dataset(X_train_scaled, y_train.values, TIME_STEPS)\n",
    "X_test_lstm, y_test_lstm = create_lstm_dataset(X_test_scaled, y_test.values, TIME_STEPS)\n",
    "\n",
    "weights_lstm = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_lstm),\n",
    "    y=y_train_lstm\n",
    ")\n",
    "class_weights_dict_lstm = dict(zip(np.unique(y_train_lstm), weights_lstm))\n",
    "\n",
    "def build_lstm_stage1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training LSTM Stage 1 (Detection) ---\")\n",
    "lstm_model_s1 = build_lstm_stage1(LSTM_INPUT_SHAPE)\n",
    "lstm_model_s1.summary()\n",
    "\n",
    "lstm_model_s1.fit(X_train_lstm, y_train_lstm,\n",
    "                  batch_size=128,\n",
    "                  epochs=20, # Increase epochs, EarlyStopping will handle it\n",
    "                  validation_split=0.1,\n",
    "                  class_weight=class_weights_dict_lstm,\n",
    "                  callbacks=[early_stopper]) # <-- ADDED CALLBACK\n",
    "                  \n",
    "print(\"\\n--- LSTM Stage 1 Evaluation ---\")\n",
    "y_pred_lstm_s1 = (lstm_model_s1.predict(X_test_lstm) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test_lstm, y_pred_lstm_s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b0504-8e4a-43a3-87da-919107629829",
   "metadata": {},
   "source": [
    "The EarlyStopping callback monitored the validation loss (val_loss) and prevented both models from overfitting.\n",
    "\n",
    "For the CNN: It stopped training after Epoch 4 and restored the weights from Epoch 1, which had the best val_loss (0.6686). This tells us the CNN model wasn't improving after the very first epoch.\n",
    "\n",
    "For the LSTM: It trained for 12 epochs, but it restored the weights from Epoch 9, which had the best val_loss (0.5089). This is exactly what we wanted. It found the \"sweet spot\" before the model started to overfit (which we saw in your previous run)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fae13-d13d-44e0-a8a2-b4a6787f7282",
   "metadata": {},
   "source": [
    "## Stage 2 - Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3efac15c-de25-4cf8-9391-58786c6b18b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data prepared with 37 common features.\n",
      "Found 18 unique fault types. Setting N_CLASSES to 21.\n",
      "Scaling data...\n",
      "Calculating class weights...\n",
      "Class weights calculated for 18 classes and applied to all 21 classes.\n",
      "\n",
      "--- Preparing CNN Data ---\n",
      "Original features: 37. Padding to 49 for a (7, 7) image.\n",
      "\n",
      "--- Training CNN Stage 2 (Classification) ---\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 5, 5, 32)          320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 2, 2, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 21)                2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,541\n",
      "Trainable params: 19,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 2.3718 - accuracy: 0.2632 - val_loss: 2.2418 - val_accuracy: 0.2716\n",
      "Epoch 2/20\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 1.8554 - accuracy: 0.3986 - val_loss: 2.0053 - val_accuracy: 0.2882\n",
      "Epoch 3/20\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 1.6771 - accuracy: 0.4447 - val_loss: 1.9001 - val_accuracy: 0.2851\n",
      "Epoch 4/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.5886 - accuracy: 0.4688 - val_loss: 1.8336 - val_accuracy: 0.3121\n",
      "Epoch 5/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.5339 - accuracy: 0.4822 - val_loss: 1.8421 - val_accuracy: 0.3063\n",
      "Epoch 6/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.4974 - accuracy: 0.4965 - val_loss: 1.7874 - val_accuracy: 0.3337\n",
      "Epoch 7/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.4613 - accuracy: 0.5054 - val_loss: 1.8250 - val_accuracy: 0.3094\n",
      "Epoch 8/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.4339 - accuracy: 0.5160 - val_loss: 1.7633 - val_accuracy: 0.3341\n",
      "Epoch 9/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.4114 - accuracy: 0.5186 - val_loss: 1.7608 - val_accuracy: 0.3295\n",
      "Epoch 10/20\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 1.3875 - accuracy: 0.5269 - val_loss: 1.7259 - val_accuracy: 0.3399\n",
      "Epoch 11/20\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 1.3762 - accuracy: 0.5309 - val_loss: 1.7431 - val_accuracy: 0.3349\n",
      "Epoch 12/20\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 1.3526 - accuracy: 0.5405 - val_loss: 1.7152 - val_accuracy: 0.3395\n",
      "Epoch 13/20\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 1.3383 - accuracy: 0.5438 - val_loss: 1.7120 - val_accuracy: 0.3407\n",
      "Epoch 14/20\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 1.3332 - accuracy: 0.5453 - val_loss: 1.6883 - val_accuracy: 0.3480\n",
      "Epoch 15/20\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 1.3179 - accuracy: 0.5478 - val_loss: 1.6732 - val_accuracy: 0.3519\n",
      "Epoch 16/20\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 1.3059 - accuracy: 0.5507 - val_loss: 1.6735 - val_accuracy: 0.3549\n",
      "Epoch 17/20\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 1.3008 - accuracy: 0.5552 - val_loss: 1.6882 - val_accuracy: 0.3457\n",
      "Epoch 18/20\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 1.2929 - accuracy: 0.5554 - val_loss: 1.6703 - val_accuracy: 0.3522\n",
      "Epoch 19/20\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 1.2867 - accuracy: 0.5589 - val_loss: 1.6657 - val_accuracy: 0.3438\n",
      "Epoch 20/20\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.2736 - accuracy: 0.5569 - val_loss: 1.6380 - val_accuracy: 0.3650\n",
      "\n",
      "--- CNN Stage 2 Evaluation ---\n",
      "1587/1587 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.27      0.17      2820\n",
      "           1       0.95      0.83      0.89      2820\n",
      "           2       1.00      0.83      0.91      2820\n",
      "           4       0.14      0.59      0.22      2820\n",
      "           5       0.19      0.11      0.14      2820\n",
      "           6       1.00      0.67      0.80      2820\n",
      "           7       0.70      0.85      0.77      2820\n",
      "           8       0.76      0.47      0.58      2820\n",
      "          10       0.41      0.32      0.36      2820\n",
      "          11       0.39      0.57      0.46      2820\n",
      "          12       0.48      0.36      0.41      2820\n",
      "          13       0.94      0.56      0.70      2820\n",
      "          14       0.95      0.79      0.86      2820\n",
      "          16       0.00      0.00      0.00      2820\n",
      "          17       0.88      0.58      0.70      2820\n",
      "          18       0.62      0.73      0.67      2820\n",
      "          19       0.19      0.01      0.01      2820\n",
      "          20       0.48      0.20      0.29      2820\n",
      "\n",
      "    accuracy                           0.49     50760\n",
      "   macro avg       0.57      0.49      0.50     50760\n",
      "weighted avg       0.57      0.49      0.50     50760\n",
      "\n",
      "\n",
      "--- Preparing LSTM Data ---\n",
      "\n",
      "--- Training LSTM Stage 2 (Classification) ---\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 10, 50)            17600     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 21)                1071      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,421\n",
      "Trainable params: 41,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "183/183 [==============================] - 8s 25ms/step - loss: 1.6103 - accuracy: 0.5075 - val_loss: 1.3019 - val_accuracy: 0.5940\n",
      "Epoch 2/20\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.7108 - accuracy: 0.7659 - val_loss: 1.0291 - val_accuracy: 0.6719\n",
      "Epoch 3/20\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.5375 - accuracy: 0.8134 - val_loss: 1.3588 - val_accuracy: 0.6804\n",
      "Epoch 4/20\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.4777 - accuracy: 0.8313 - val_loss: 1.4603 - val_accuracy: 0.6955\n",
      "Epoch 5/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8488Restoring model weights from the end of the best epoch: 2.\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.4198 - accuracy: 0.8488 - val_loss: 1.4283 - val_accuracy: 0.6804\n",
      "Epoch 5: early stopping\n",
      "\n",
      "--- LSTM Stage 2 Evaluation ---\n",
      "1586/1586 [==============================] - 9s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.81      0.27      2810\n",
      "           1       0.91      0.84      0.87      2820\n",
      "           2       0.96      0.84      0.89      2820\n",
      "           4       0.90      0.84      0.87      2820\n",
      "           5       0.29      0.18      0.22      2820\n",
      "           6       0.94      0.42      0.58      2820\n",
      "           7       0.65      0.85      0.74      2820\n",
      "           8       0.87      0.57      0.69      2820\n",
      "          10       0.31      0.20      0.24      2820\n",
      "          11       0.89      0.62      0.73      2820\n",
      "          12       0.62      0.57      0.60      2820\n",
      "          13       0.75      0.62      0.68      2820\n",
      "          14       0.99      0.84      0.91      2820\n",
      "          16       0.23      0.01      0.03      2820\n",
      "          17       0.78      0.80      0.79      2820\n",
      "          18       0.88      0.46      0.60      2820\n",
      "          19       0.94      0.72      0.82      2820\n",
      "          20       0.62      0.63      0.63      2820\n",
      "\n",
      "    accuracy                           0.60     50750\n",
      "   macro avg       0.71      0.60      0.62     50750\n",
      "weighted avg       0.71      0.60      0.62     50750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_factors(n):\n",
    "    for i in range(int(np.sqrt(n)), 0, -1):\n",
    "        if n % i == 0:\n",
    "            return (n // i, i)\n",
    "    return (n, 1)\n",
    "\n",
    "def create_cnn_dataset(X, img_rows, img_cols):\n",
    "    if X.shape[1] != img_rows * img_cols:\n",
    "        raise ValueError(f\"Cannot reshape {X.shape[1]} features into ({img_rows}, {img_cols})\")\n",
    "    return X.reshape((X.shape[0], img_rows, img_cols, 1))\n",
    "\n",
    "def create_lstm_dataset(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "    \n",
    "def pad_features(X, target_features):\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    pad_width = target_features - num_features\n",
    "    if pad_width < 0:\n",
    "        return X[:, :target_features]\n",
    "    padding = np.zeros((num_samples, pad_width))\n",
    "    return np.concatenate([X, padding], axis=1)\n",
    "\n",
    "# --- 1. DATA PREPARATION (for Multiclass) ---\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('modified_data_train.csv')\n",
    "test_data  = pd.read_csv('modified_data_test.csv')\n",
    "\n",
    "drop_cols = ['faultNumber', 'simulationRun', 'sample']\n",
    "\n",
    "# --- [FIX 1: Cast labels to int] ---\n",
    "X_train = train_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_train_labels = train_data['faultNumber'].astype(int)\n",
    "\n",
    "X_test = test_data.drop(columns=drop_cols, errors='ignore')\n",
    "y_test_labels = test_data['faultNumber'].astype(int)\n",
    "# --- [END OF FIX 1] ---\n",
    "\n",
    "common_cols = sorted(list(set(X_train.columns).intersection(set(X_test.columns))))\n",
    "\n",
    "X_train = X_train[common_cols]\n",
    "X_test  = X_test[common_cols]\n",
    "\n",
    "print(f\"Data prepared with {len(common_cols)} common features.\")\n",
    "\n",
    "# Determine number of classes\n",
    "all_labels = pd.concat([y_train_labels, y_test_labels])\n",
    "N_CLASSES = int(all_labels.max()) + 1 \n",
    "\n",
    "print(f\"Found {len(all_labels.unique())} unique fault types. Setting N_CLASSES to {N_CLASSES}.\")\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = to_categorical(y_train_labels, num_classes=N_CLASSES)\n",
    "y_test_cat = to_categorical(y_test_labels, num_classes=N_CLASSES)\n",
    "\n",
    "\n",
    "# --- 2. SCALING ---\n",
    "print(\"Scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "N_FEATURES = X_train_scaled.shape[1]\n",
    "\n",
    "# --- [FIX 2: Create a full class_weight dictionary] ---\n",
    "print(\"Calculating class weights...\")\n",
    "\n",
    "# Get the unique integer labels from the training set\n",
    "unique_labels = np.unique(y_train_labels)\n",
    "\n",
    "# Calculate weights only for the classes present\n",
    "weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=unique_labels,\n",
    "    y=y_train_labels\n",
    ")\n",
    "\n",
    "# Create a full dictionary mapping ALL classes (0 to N_CLASSES-1) to 1.0\n",
    "class_weights_dict = {i: 1.0 for i in range(N_CLASSES)}\n",
    "\n",
    "# Update the dictionary with the calculated weights for the classes we have\n",
    "calculated_dict = dict(zip(unique_labels, weights))\n",
    "class_weights_dict.update(calculated_dict)\n",
    "\n",
    "print(f\"Class weights calculated for {len(unique_labels)} classes and applied to all {N_CLASSES} classes.\")\n",
    "# --- [END OF FIX 2] ---\n",
    "\n",
    "# --- DEFINE EARLY STOPPING ---\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. MODEL 1: CNN FOR STAGE 2 (CLASSIFICATION) ---\n",
    "\n",
    "print(\"\\n--- Preparing CNN Data ---\")\n",
    "TARGET_DIM = int(np.ceil(np.sqrt(N_FEATURES)))\n",
    "IMG_ROWS, IMG_COLS = TARGET_DIM, TARGET_DIM\n",
    "TARGET_FEATURES = TARGET_DIM * TARGET_DIM\n",
    "\n",
    "print(f\"Original features: {N_FEATURES}. Padding to {TARGET_FEATURES} for a ({IMG_ROWS}, {IMG_COLS}) image.\")\n",
    "\n",
    "X_train_padded = pad_features(X_train_scaled, TARGET_FEATURES)\n",
    "X_test_padded = pad_features(X_test_scaled, TARGET_FEATURES)\n",
    "\n",
    "CNN_INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "\n",
    "X_train_cnn = create_cnn_dataset(X_train_padded, IMG_ROWS, IMG_COLS)\n",
    "X_test_cnn = create_cnn_dataset(X_test_padded, IMG_ROWS, IMG_COLS)\n",
    "\n",
    "\n",
    "def build_cnn_stage2(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training CNN Stage 2 (Classification) ---\")\n",
    "cnn_model_s2 = build_cnn_stage2(CNN_INPUT_SHAPE, N_CLASSES)\n",
    "cnn_model_s2.summary()\n",
    "\n",
    "# Pass the fixed, full class_weights_dict\n",
    "cnn_model_s2.fit(X_train_cnn, y_train_cat,\n",
    "                 batch_size=128,\n",
    "                 epochs=20,\n",
    "                 validation_split=0.1,\n",
    "                 class_weight=class_weights_dict,\n",
    "                 callbacks=[early_stopper])\n",
    "\n",
    "print(\"\\n--- CNN Stage 2 Evaluation ---\")\n",
    "y_pred_cnn_s2_probs = cnn_model_s2.predict(X_test_cnn)\n",
    "y_pred_cnn_s2_labels = np.argmax(y_pred_cnn_s2_probs, axis=1)\n",
    "print(classification_report(y_test_labels, y_pred_cnn_s2_labels, zero_division=0))\n",
    "\n",
    "\n",
    "# --- 4. MODEL 2: LSTM FOR STAGE 2 (CLASSIFICATION) ---\n",
    "\n",
    "print(\"\\n--- Preparing LSTM Data ---\")\n",
    "TIME_STEPS = 10\n",
    "LSTM_INPUT_SHAPE = (TIME_STEPS, N_FEATURES) \n",
    "\n",
    "X_train_lstm, y_train_lstm_cat = create_lstm_dataset(X_train_scaled, y_train_cat, TIME_STEPS)\n",
    "X_test_lstm, y_test_lstm_cat = create_lstm_dataset(X_test_scaled, y_test_cat, TIME_STEPS)\n",
    "\n",
    "_, y_test_lstm_labels = create_lstm_dataset(X_test_scaled, y_test_labels.values, TIME_STEPS)\n",
    "\n",
    "# --- [FIX 2 applied to LSTM weights] ---\n",
    "_, y_train_lstm_labels = create_lstm_dataset(X_train_scaled, y_train_labels.values, TIME_STEPS)\n",
    "\n",
    "# Get unique integer labels from the time-stepped training data\n",
    "unique_labels_lstm = np.unique(y_train_lstm_labels)\n",
    "\n",
    "weights_lstm = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=unique_labels_lstm,\n",
    "    y=y_train_lstm_labels\n",
    ")\n",
    "\n",
    "# Create a full dictionary for all classes\n",
    "class_weights_dict_lstm = {i: 1.0 for i in range(N_CLASSES)}\n",
    "\n",
    "# Update with calculated weights\n",
    "calculated_dict_lstm = dict(zip(unique_labels_lstm, weights_lstm))\n",
    "class_weights_dict_lstm.update(calculated_dict_lstm)\n",
    "# --- [END OF FIX 2] ---\n",
    "\n",
    "\n",
    "def build_lstm_stage2(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training LSTM Stage 2 (Classification) ---\")\n",
    "lstm_model_s2 = build_lstm_stage2(LSTM_INPUT_SHAPE, N_CLASSES)\n",
    "lstm_model_s2.summary()\n",
    "\n",
    "# Pass the fixed, full class_weights_dict_lstm\n",
    "lstm_model_s2.fit(X_train_lstm, y_train_lstm_cat,\n",
    "                  batch_size=128,\n",
    "                  epochs=20,\n",
    "                  validation_split=0.1,\n",
    "                  class_weight=class_weights_dict_lstm,\n",
    "                  callbacks=[early_stopper])\n",
    "                  \n",
    "print(\"\\n--- LSTM Stage 2 Evaluation ---\")\n",
    "y_pred_lstm_s2_probs = lstm_model_s2.predict(X_test_lstm)\n",
    "y_pred_lstm_s2_labels = np.argmax(y_pred_lstm_s2_probs, axis=1)\n",
    "\n",
    "print(classification_report(y_test_lstm_labels, y_pred_lstm_s2_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99406f2-e4b0-49c0-838c-b3693611c824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
